{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT-R.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUF8gJAF1GCq",
        "colab_type": "code",
        "outputId": "419211e2-5724-4cce-c1f4-afb06d45c5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "## From keras import \n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcxXRhe04gQB",
        "colab_type": "code",
        "outputId": "96140a0e-e7cf-47f2-931d-494df5e524ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMF-1lSr1GC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load the data\n",
        "file1 = open(\"gdrive/My Drive/Colab Notebooks/data/eng2vi/train.en\", encoding = \"utf8\")   # Load English Data\n",
        "english = file1.readlines()\n",
        "\n",
        "file2 = open(\"gdrive/My Drive/Colab Notebooks/data/eng2vi/train.vi\", encoding = \"utf8\")   # Load German Data\n",
        "vitn = file2.readlines()\n",
        "\n",
        "### Now add a start and end marker for the destination language. \n",
        "for i in range(0,len(vitn)):\n",
        "    vitn[i] = \"starttt \" + vitn[i] + \" enddd\"\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1phRPPl1GDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = 10000 ## Most frequent 10,000 words for tokenizing. Make it 30k if dataset is large\n",
        "\n",
        "class TokenizerWrap(Tokenizer):\n",
        "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, padding,\n",
        "                 reverse=False, num_words=None):\n",
        "        \"\"\"\n",
        "        :param texts: List of strings. This is the data-set.\n",
        "        :param padding: Either 'post' or 'pre' padding.\n",
        "        :param reverse: Boolean whether to reverse token-lists.\n",
        "        :param num_words: Max number of words to use.\n",
        "        \"\"\"\n",
        "\n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "        self.fit_on_texts(texts)\n",
        "\n",
        "        # Create inverse lookup from integer-tokens to words.\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        if reverse:\n",
        "    \n",
        "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "          \n",
        "            truncating = 'post'\n",
        "\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "\n",
        "        self.max_tokens = np.mean(self.num_tokens) \\\n",
        "                          + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                           padding=padding,\n",
        "                                           truncating=truncating)\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        \n",
        " \n",
        "        text = \" \".join(words)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
        "\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "        if reverse:\n",
        "            tokens = np.flip(tokens, axis=1)\n",
        "\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "\n",
        "            truncating = 'post'\n",
        "\n",
        "        if padding:\n",
        "            tokens = pad_sequences(tokens,\n",
        "                                   maxlen=self.max_tokens,\n",
        "                                   padding='pre',\n",
        "                                   truncating=truncating)\n",
        "        return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBGsoA6u1GDK",
        "colab_type": "code",
        "outputId": "81b72c61-ecee-403b-ee18-b358fb410fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "### Now tokenize the datasets\n",
        "\n",
        "tokenizer_eng = TokenizerWrap(texts=english,\n",
        "                              padding='pre',\n",
        "                              reverse=True,\n",
        "                              num_words=num_words)     \n",
        " \n",
        "tokenizer_vitn = TokenizerWrap(texts=vitn,\n",
        "                               padding='post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)\n",
        "\n",
        "### This is to reduce the memory used for tokenizing the words.\n",
        "tokens_eng = tokenizer_eng.tokens_padded\n",
        "tokens_vitn = tokenizer_vitn.tokens_padded\n",
        "print(tokens_eng.shape)\n",
        "print(tokens_vitn.shape)\n",
        "## Tokenizing done here ####\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(133317, 44)\n",
            "(133317, 59)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S1MEQHD1GDQ",
        "colab_type": "code",
        "outputId": "6a9476ea-7259-475f-a7fd-0a83c3673ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "encoder_input_data = tokens_eng\n",
        "\n",
        "decoder_input_data = tokens_vitn[:, :-1]       # They are in reverse format, so reverse them\n",
        "decoder_output_data = tokens_vitn[:, 1:]       # First 'start' marker is time stepped in output\n",
        "\n",
        "##### Neural Network #####\n",
        "\n",
        "encoder_input = Input(shape=(None, ), name='encoder_input')\n",
        "\n",
        "embedding_size = 128\n",
        "state_size = 512\n",
        "\n",
        "encoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='encoder_embedding')\n",
        "\n",
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
        "                   return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
        "                   return_sequences=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
        "                   return_sequences=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvvWvt3q1GDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_encoder():\n",
        "    # Start the neural network with its input-layer.\n",
        "    net = encoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = encoder_embedding(net)\n",
        "\n",
        "    # Connect all the GRU-layers.\n",
        "    net = encoder_gru1(net)\n",
        "    net = encoder_gru2(net)\n",
        "    net = encoder_gru3(net)\n",
        "\n",
        "    # This is the output of the encoder.\n",
        "    encoder_output = net\n",
        "    \n",
        "    return encoder_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39H93rtE1GDY",
        "colab_type": "code",
        "outputId": "32c96825-e672-43fd-dae7-68a21cfe31c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "## If we use LSTM's instead of GRU's at this place, We cannot take output the way we have taken now.     \n",
        "encoder_output = connect_encoder()\n",
        "\n",
        "\n",
        "######### Similarly build the decoder\n",
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                              name='decoder_initial_state')\n",
        "\n",
        "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')\n",
        "\n",
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)\n",
        "\n",
        "decoder_dense = Dense(num_words, activation='linear', name='decoder_output')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ_toSul1GDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_decoder(initial_state):\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "\n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    \n",
        "    # Connect all the GRU-layers.\n",
        "    net = decoder_gru1(net, initial_state=initial_state)\n",
        "    net = decoder_gru2(net, initial_state=initial_state)\n",
        "    net = decoder_gru3(net, initial_state=initial_state)\n",
        "\n",
        "    # Connect the final dense layer that converts to\n",
        "    # one-hot encoded arrays.\n",
        "    decoder_output = decoder_dense(net)\n",
        "    \n",
        "    return decoder_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc_3Abp51GDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "######## Now connect all layers and create the model.\n",
        "    \n",
        "decoder_output = connect_decoder(initial_state=encoder_output)\n",
        "\n",
        "model_train = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
        "\n",
        "model_encoder = Model(inputs=[encoder_input],\n",
        "                      outputs=[encoder_output])\n",
        "\n",
        "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
        "\n",
        "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
        "                      outputs=[decoder_output])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAftvi-H1GDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the cross-entropy loss between y_true and y_pred.\n",
        "    \n",
        "    y_true is a 2-rank tensor with the desired output.\n",
        "    The shape is [batch_size, sequence_length] and it\n",
        "    contains sequences of integer-tokens.\n",
        "\n",
        "    y_pred is the decoder's output which is a 3-rank tensor\n",
        "    with shape [batch_size, sequence_length, num_words]\n",
        "    so that for each sequence in the batch there is a one-hot\n",
        "    encoded array of length num_words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the loss. This outputs a\n",
        "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "\n",
        "    # Keras may reduce this across the first axis (the batch)\n",
        "    # but the semantics are unclear, so to be sure we use\n",
        "    # the loss across the entire 2-rank tensor, we reduce it\n",
        "    # to a single scalar with the mean function.\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICJ4M7OS1GDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Compile the model\n",
        "\n",
        "optimizer = RMSprop(lr=1e-3)    ### Adam / Adagrad dosent work well with RNN's\n",
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "model_train.compile(optimizer=optimizer,\n",
        "                    loss=sparse_cross_entropy,\n",
        "                    target_tensors=[decoder_target])\n",
        "\n",
        "### Callbacks\n",
        "path_checkpoint = '21_checkpoint.keras'\n",
        "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                      monitor='val_loss',\n",
        "                                      verbose=1,\n",
        "                                      save_weights_only=True,\n",
        "                                      save_best_only=True)\n",
        "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                                        patience=3, verbose=1)\n",
        "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
        "                                   histogram_freq=0,\n",
        "                                   write_graph=False)\n",
        "callbacks = [callback_early_stopping,\n",
        "             callback_checkpoint,\n",
        "             callback_tensorboard]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I64IwRX1GDu",
        "colab_type": "code",
        "outputId": "5fcacdcc-0908-4149-d9a2-05595391c1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "####### Train the model\n",
        "\n",
        "x_data = {\n",
        "    'encoder_input': encoder_input_data,\n",
        "    'decoder_input': decoder_input_data\n",
        "}\n",
        "\n",
        "y_data = {\n",
        "    'decoder_output': decoder_output_data\n",
        "}\n",
        "\n",
        "validation_split = 10000 / len(encoder_input_data)\n",
        "print (validation_split)\n",
        "\n",
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=512,\n",
        "                epochs=4,\n",
        "                validation_split=validation_split,\n",
        "                callbacks = callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07500918862560664\n",
            "Train on 123317 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "123317/123317 [==============================] - ETA: 0s - loss: 2.6074 \n",
            "Epoch 00001: val_loss improved from inf to 2.39205, saving model to 21_checkpoint.keras\n",
            "123317/123317 [==============================] - 10470s 85ms/sample - loss: 2.6074 - val_loss: 2.3921\n",
            "Epoch 2/4\n",
            "123317/123317 [==============================] - ETA: 0s - loss: 2.1933 \n",
            "Epoch 00002: val_loss improved from 2.39205 to 2.17050, saving model to 21_checkpoint.keras\n",
            "123317/123317 [==============================] - 10515s 85ms/sample - loss: 2.1933 - val_loss: 2.1705\n",
            "Epoch 3/4\n",
            "123317/123317 [==============================] - ETA: 0s - loss: 1.9723 \n",
            "Epoch 00003: val_loss improved from 2.17050 to 2.00525, saving model to 21_checkpoint.keras\n",
            "123317/123317 [==============================] - 10538s 85ms/sample - loss: 1.9723 - val_loss: 2.0052\n",
            "Epoch 4/4\n",
            "123317/123317 [==============================] - ETA: 0s - loss: 1.8233 \n",
            "Epoch 00004: val_loss improved from 2.00525 to 1.90707, saving model to 21_checkpoint.keras\n",
            "123317/123317 [==============================] - 10546s 86ms/sample - loss: 1.8233 - val_loss: 1.9071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc272dcb320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKBDd9G0xWBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    validation_split = 10000 / len(encoder_input_data)\n",
        "    print (validation_split)\n",
        "\n",
        "    model_train.fit(x=x_data,\n",
        "                    y=y_data,\n",
        "                    batch_size=512,\n",
        "                    epochs=4,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks = callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YoLzPN51GDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mark_start = 'starttt'\n",
        "mark_end = 'enddd'\n",
        "token_start = tokenizer_vitn.word_index[mark_start.strip()]\n",
        "token_end = tokenizer_vitn.word_index[mark_end.strip()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNProe5JbeNf",
        "colab_type": "code",
        "outputId": "9570fcf6-5daa-4c79-caeb-9e2e52e5612e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "##### Save the trained model in Colab\n",
        "MODEL_PATH = '/content/gdrive/My Drive/Colab Notebooks/model/'\n",
        "model_train.save(MODEL_PATH + 'training_model.h5')\n",
        "\n",
        "'''\n",
        "# Recreate the exact same model, including its weights and the optimizer\n",
        "new_model_train = tf.keras.models.load_model(MODEL_PATH + 'Copy_training_model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "new_model_train.summary()\n",
        "\n",
        "\n",
        "loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
        "\n",
        "from google.colab import files\n",
        "!pip install -U -q PyDrive\n",
        "!pip install google-cloud-*\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# create on Colab directory\n",
        "model_train.save(MODEL_PATH + 'training_model.h5')    \n",
        "model_file = drive.CreateFile({'title' : 'training_model.h5'})\n",
        "model_file.SetContentFile(MODEL_PATH + 'training_model.h5')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Recreate the exact same model, including its weights and the optimizer\\nnew_model_train = tf.keras.models.load_model(MODEL_PATH + 'Copy_training_model.h5')\\n\\n# Show the model architecture\\nnew_model_train.summary()\\n\\n\\nloss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\\nprint('Restored model, accuracy: {:5.2f}%'.format(100*acc))\\n\\nfrom google.colab import files\\n!pip install -U -q PyDrive\\n!pip install google-cloud-*\\nfrom pydrive.auth import GoogleAuth\\nfrom pydrive.drive import GoogleDrive\\nfrom google.colab import auth\\nfrom oauth2client.client import GoogleCredentials\\n\\n\\n\\nauth.authenticate_user()\\ngauth = GoogleAuth()\\ngauth.credentials = GoogleCredentials.get_application_default()\\ndrive = GoogleDrive(gauth)\\n\\n# create on Colab directory\\nmodel_train.save(MODEL_PATH + 'training_model.h5')    \\nmodel_file = drive.CreateFile({'title' : 'training_model.h5'})\\nmodel_file.SetContentFile(MODEL_PATH + 'training_model.h5')\\nmodel_file.Upload()\\n\\n# download to google drive\\ndrive.CreateFile({'id': model_file.get('id')})\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26jFJA6p1GD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(input_text, true_output_text=None):\n",
        "    \"\"\"Translate a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_eng.text_to_tokens(text=input_text,\n",
        "                                                reverse=True,\n",
        "                                                padding=True)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state = model_encoder.predict(input_tokens)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens = tokenizer_vitn.max_tokens\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens,\n",
        "    # but the decoder-model expects a batch of sequences.\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "    \n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_initial_state': initial_state,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict(), but it would make the code\n",
        "        # much more complicated.\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_vitn.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "     # Print the input-text.\n",
        "    print(\"Input text:\")\n",
        "    print(input_text)\n",
        "    print()\n",
        "\n",
        "    # Print the translated output-text.\n",
        "    print(\"Translated text:\")\n",
        "    print(output_text)\n",
        "    print()\n",
        "\n",
        "    # Optionally print the true translated text.\n",
        "    if true_output_text is not None:\n",
        "        print(\"True output text:\")\n",
        "        print(true_output_text)\n",
        "        print()\n",
        "    \n",
        "    return input_text, output_text, true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4E4ORHI1GD7",
        "colab_type": "code",
        "outputId": "7b20ab3b-389b-4165-ed8b-e15f26396c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "idx = 7\n",
        "input_text, output_text, true_output_text = translate(input_text=english[idx],\n",
        "          true_output_text=vitn[idx])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "They wrote almost a thousand pages on the topic .\n",
            "\n",
            "\n",
            "Translated text:\n",
            " họ đã bắt đầu với một người phụ nữ enddd\n",
            "\n",
            "True output text:\n",
            "starttt Họ viết gần 1000 trang về chủ đề này .\n",
            " enddd\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkmmRLw0MfnV",
        "colab_type": "code",
        "outputId": "2a9a40de-329d-4dc4-8d65-b43694ab5a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "translate(input_text = 'This was an amazing day')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:\n",
            "This was an amazing day\n",
            "\n",
            "Translated text:\n",
            " đây là một câu chuyện enddd\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('This was an amazing day', ' đây là một câu chuyện enddd', None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5j5rndxU8Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate1(input_text, true_output_text=None):\n",
        "    \"\"\"Translate a single text-string.\"\"\"\n",
        "\n",
        "    # Convert the input-text to integer-tokens.\n",
        "    # Note the sequence of tokens has to be reversed.\n",
        "    # Padding is probably not necessary.\n",
        "    input_tokens = tokenizer_eng.text_to_tokens(text=input_text,\n",
        "                                                reverse=True,\n",
        "                                                padding=True)\n",
        "    \n",
        "    # Get the output of the encoder's GRU which will be\n",
        "    # used as the initial state in the decoder's GRU.\n",
        "    # This could also have been the encoder's final state\n",
        "    # but that is really only necessary if the encoder\n",
        "    # and decoder use the LSTM instead of GRU because\n",
        "    # the LSTM has two internal states.\n",
        "    initial_state = model_encoder.predict(input_tokens)\n",
        "\n",
        "    # Max number of tokens / words in the output sequence.\n",
        "    max_tokens = tokenizer_vitn.max_tokens\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens,\n",
        "    # but the decoder-model expects a batch of sequences.\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "    \n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_initial_state': initial_state,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict(), but it would make the code\n",
        "        # much more complicated.\n",
        "\n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = model_decoder.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        \n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer_vitn.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # Sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "     # Print the input-text\n",
        "    \n",
        "    return input_text, output_text, true_output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h66U8Eql1GEA",
        "colab_type": "code",
        "outputId": "5ff6d111-4a25-4f78-ed81-4404a25b6b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "sm = SmoothingFunction()\n",
        "score = sentence_bleu([output_text], true_output_text, smoothing_function=sm.method1)\n",
        "print(score)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.15192495332691272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPWiTPmNEtda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "      ## Load the data\n",
        "    file1 = open(\"/content/gdrive/My Drive/Colab Notebooks/data/eng2vi/tst2013.en\", encoding = \"utf8\")   # Load English Data\n",
        "    english_test = file1.readlines()\n",
        "\n",
        "    file2 = open(\"/content/gdrive/My Drive/Colab Notebooks/data/eng2vi/tst2013.vi\", encoding = \"utf8\")   # Load German Data\n",
        "    vitn_test = file2.readlines()\n",
        "\n",
        "    ### Now add a start and end marker for the destination language. \n",
        "    for i in range(0,len(vitn_test)):\n",
        "        vitn_test[i] = \"starttt \" + vitn_test[i] + \" enddd\"\n",
        "    \n",
        "    #count = 0\n",
        "    scores_list = []\n",
        "    print('Test : ')\n",
        "    for idx in range(0,20): # Doing for 100 lines\n",
        "        input_text, output_text, true_output_text = translate1(input_text=english_test[idx],true_output_text=vitn_test[idx])\n",
        "        scor = sentence_bleu([output_text], true_output_text, smoothing_function=sm.method1)\n",
        "        scores_list.append(scor)\n",
        "        print(output_text)\n",
        "        #print(scor)\n",
        "        \n",
        "    BLEU_average = sum(scores_list)/ 20\n",
        "    print (\"The BLEU average score for the test_data = \", BLEU_average)\n",
        "    #print(count)\n",
        "    \n",
        "    return BLEU_average\n",
        "   \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdtPLFozUbwX",
        "colab_type": "code",
        "outputId": "ef8c4d3b-c3b8-4eef-aa8f-9803cdc7cbe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "BLEU_average = test()\n",
        "\n",
        "print('BLEU score in % = ', BLEU_average * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test : \n",
            " khi tôi nói với tôi rằng tôi đã nói với bạn rằng quot tôi có thể nói quot tôi không thể nói quot tôi có thể nói quot tôi không thể nói quot enddd\n",
            " và tôi đã làm việc enddd\n",
            " trong năm 2009 chúng tôi đã làm việc với một người phụ nữ với những người mà họ đã làm việc với những người mà họ đã làm việc với những người mà họ đã làm việc với những người mà họ đã làm việc với những người khác như thế này enddd\n",
            " điều này tôi muốn làm việc với một cách khác biệt với các bạn thấy một vài người trong số các bạn có thể thấy một số người trong số các nước khác và tôi có thể làm được điều đó enddd\n",
            " khi tôi bắt đầu với một câu chuyện về tôi và tôi đã làm việc với một người phụ nữ và tôi đã làm việc với một người phụ nữ và tôi đã làm việc với một người phụ nữ enddd\n",
            " tôi không thể nói rằng tôi không thể làm được điều đó enddd\n",
            " nhưng khi tôi bắt đầu với một câu chuyện về một câu chuyện về một câu chuyện về một câu chuyện về một câu chuyện enddd\n",
            " quot không phải là một người phụ nữ không phải là một người phụ nữ không phải là một người phụ nữ và ông ấy không phải là một người phụ nữ và ông ấy không phải là một người phụ nữ không phải là một người phụ nữ enddd\n",
            " chúng tôi có một cái gì đó chúng tôi có thể làm được điều này và chúng tôi có thể làm gì đó enddd\n",
            " tôi đã làm việc enddd\n",
            " đây là một bức ảnh tôi đã làm việc với một người khác tôi có thể làm việc với một người khác enddd\n",
            " khi tôi bắt đầu với một câu hỏi tôi có thể làm việc với một người bạn có thể làm việc với tôi và tôi không thể làm gì enddd\n",
            " một người phụ nữ đã được lắng nghe về những người phụ nữ và ông ấy đã làm việc với một người đàn ông enddd\n",
            " nhưng bạn không thể làm được những gì chúng ta đã làm được những gì chúng ta đã làm được những gì chúng ta đã làm được những gì chúng ta đã làm được enddd\n",
            " một số người trong số các nhà văn hoá enddd\n",
            " chúng tôi có một số người trong số những người khác và họ có thể làm được những gì họ có thể làm được điều đó và họ có thể làm được điều đó enddd\n",
            " nhưng nó có thể là một phần của tôi và tôi có thể làm việc với một người phụ nữ và tôi có thể làm việc với một người phụ nữ và tôi đã làm việc với một người phụ nữ và tôi đã làm việc với một người phụ nữ enddd\n",
            " tôi nói quot là một người không thể làm được điều đó enddd\n",
            " đây là một ví dụ về những người phụ nữ này đã được sử dụng những người phụ nữ enddd\n",
            " đây là một ví dụ về những người mà chúng tôi đang làm việc với những người khác trong những người này và họ đã làm được những gì họ đã làm được enddd\n",
            "The BLEU average score for the test_data =  0.2491269449125943\n",
            "BLEU score in % =  24.91269449125943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49l72aPYEyFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _get_user_input():\n",
        "    \"\"\" Get user's input, which will be transformed into encoder input later \"\"\"\n",
        "    print(\"> \", end=\"\")\n",
        "    sys.stdout.flush()\n",
        "    return sys.stdin.readline()\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Neural Machine Translation (NMT) using Recurrent Neural Network (RNN)')\n",
        "    parser.add_argument('function', nargs=1, type=str, choices={'train', 'test', 'translate'},\n",
        "                    help='enter train/test/translate accordingly to the NMT')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.function[0] == 'train':\n",
        "        print(\"Training Model\")\n",
        "        train()\n",
        "    elif args.function[0] == 'test':\n",
        "        print(\"Loading Model to Test...\")\n",
        "        test()\n",
        "    elif args.function[0] == 'translate':\n",
        "        print(\"Loading Model to translate\")\n",
        "        max_length = 50\n",
        "        while True:\n",
        "            line = _get_user_input()\n",
        "            if len(line) > 0 and line[-1] == '\\n':\n",
        "                line = line[:-1]\n",
        "            if line == '':\n",
        "                break\n",
        "\n",
        "            if (len(line) > max_length):\n",
        "                print('Max length I can handle is:', max_length)\n",
        "                line = _get_user_input()\n",
        "                continue\n",
        "            input_text, output_text, true_output_text = translate1(input_text=line)\n",
        "            print('Input : ' + input_text + '\\n')\n",
        "            print('Output : ' + output_text + '\\n')\n",
        "        print('=============================================\\n')\n",
        "    else:\n",
        "        print('\\nPlease enter a valid command')\n",
        "        parser.print_help()\n",
        "\n",
        "    exit()\n",
        "\n",
        "if __name__ == '__main__' : \n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}